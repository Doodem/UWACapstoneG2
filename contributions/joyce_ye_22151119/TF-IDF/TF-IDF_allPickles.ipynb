{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b9fe3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab0b085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, contraction_mapping):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.contraction_mapping = contraction_mapping\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if text is None:\n",
    "            # 处理文本为 None 的情况，例如返回一个空字符串\n",
    "            return \"\"\n",
    "\n",
    "        text = self._to_lower(text)\n",
    "        text = self._expand_contractions(text)\n",
    "        text = self._remove_punctuation(text)\n",
    "        tokens = self._tokenize(text)\n",
    "        tokens = self._lemmatize_tokens(tokens)\n",
    "        tokens = self._remove_stop_words(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def _to_lower(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def _expand_contractions(self, text):\n",
    "        for word, new_word in self.contraction_mapping.items():\n",
    "            text = text.replace(word, new_word)\n",
    "        return text\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def _lemmatize_tokens(self, tokens):\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    def _remove_stop_words(self, tokens):\n",
    "        return [token for token in tokens if token not in self.stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a62924e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "\n",
    "class Tender:\n",
    "    def __init__(self, reference):\n",
    "        self.reference = reference\n",
    "        self.file_map = {}\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "    # convert from binary string if needed\n",
    "        if type(text) == bytes:\n",
    "            text = text.decode(\"utf-8\")\n",
    "        text = re.sub(\"[^a-zA-z0-9.,]\", \" \", text)\n",
    "        text = re.sub(\"\\\\\\\\\", \" \", text) \n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "        text = re.sub(\"\\.+\", \".\", text)\n",
    "        return text   \n",
    "    \n",
    "    def add(self, file_name, content):\n",
    "        if content == None:\n",
    "            print(f\"Warning: None content for ref:{self.reference}, fname:{file_name}\")\n",
    "        else:\n",
    "            content = self.clean_text(content)\n",
    "            \n",
    "        if file_name in self.file_map:\n",
    "            # hopefully wont happen\n",
    "            print(f\"Warning: duplicate file name added for ref:{self.reference} fname:{file_name}\")\n",
    "        else:\n",
    "            self.file_map[file_name] = content\n",
    "    \n",
    "    def save(self):\n",
    "        with open(f\"{self.reference}.pickle\", 'wb') as file_handle:\n",
    "            pickle.dump(self.file_map, file_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "           \n",
    "    @staticmethod\n",
    "    def __correct_handle(reference):\n",
    "        if \".pickle\" in reference: # assume its a fpath, dont change\n",
    "            return reference\n",
    "        else: # try ref.pickle\n",
    "            return f\"{reference}.pickle\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def exists(reference):\n",
    "        return os.path.exists(Tender._Tender__correct_handle(reference))\n",
    "            \n",
    "    @staticmethod\n",
    "    def load(reference):\n",
    "        if Tender.exists(reference):\n",
    "            with open(Tender._Tender__correct_handle(reference), 'rb') as file_handle:\n",
    "                t = Tender(reference)\n",
    "                t.file_map = pickle.load(file_handle)\n",
    "                return t\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0448dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"C:/Users/yfr/Downloads/Capstone/data/tender_raw\"\n",
    "os.chdir(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ef34db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 Common Words:\n"
     ]
    }
   ],
   "source": [
    "# list top 50 common words\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# read \"all_file_content.txt\" \n",
    "with open(\"all_file_content.txt\", \"r\", encoding=\"utf-8\") as input_file:\n",
    "    text_data = input_file.readlines()\n",
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=50,  \n",
    "    stop_words=\"english\",  \n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\", \n",
    ")\n",
    "\n",
    "\n",
    "word_counts = count_vectorizer.fit_transform(text_data)\n",
    "\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "word_counts_df = pd.DataFrame(data=word_counts.toarray(), columns=feature_names)\n",
    "\n",
    "\n",
    "top_50_common_words = word_counts_df.sum(axis=0).sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38608efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "file_names = os.listdir(directory_path)\n",
    "\n",
    "def calculate_tfidf(file_names, top_50_common_words):\n",
    "    text_processor = TextProcessor(contraction_mapping={})\n",
    "    text_data_processed = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        if file_name.endswith('.txt'):\n",
    "            with open(file_name, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "                content = txt_file.read()\n",
    "                \n",
    "                # Text preprocessing\n",
    "                preprocessed_content = \" \".join(text_processor.preprocess_text(content))\n",
    "                \n",
    "                # Remove common words\n",
    "                for common_word in top_50_common_words.index:\n",
    "                    preprocessed_content = preprocessed_content.replace(common_word, \"\")\n",
    "                \n",
    "                text_data_processed.append(preprocessed_content)\n",
    "\n",
    "    # Combine into one corpus\n",
    "    corpus = \"\\n\".join(text_data_processed)\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=30)\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([corpus])\n",
    "\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Save TF-IDF values\n",
    "    tfidf_df = pd.DataFrame(data=tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f58c3e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 TF-IDF Terms:\n",
      "       area  condition     cost    detail  document      door   drawing  \\\n",
      "0  0.171187   0.173271  0.12886  0.169471  0.145646  0.151471  0.214484   \n",
      "\n",
      "         er   finish      form  ...   product  requirement  schedule  \\\n",
      "0  0.202853  0.13389  0.129162  ...  0.140816     0.344155  0.142554   \n",
      "\n",
      "   standard       sub   surface    system       ter      test    within  \n",
      "0  0.163235  0.168438  0.164832  0.306657  0.145444  0.133112  0.153049  \n",
      "\n",
      "[1 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "result_df = calculate_tfidf(file_names, top_50_common_words)\n",
    "print(\"Top 30 TF-IDF Terms:\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88594108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
