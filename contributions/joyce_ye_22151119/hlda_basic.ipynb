{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "191a927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tomotopy as tp\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba38558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 15980 , Vocab size: 17000 , Num words: 3303761\n",
      "Removed top words: ['thi', 'also', 'first', 'one', 'new', 'state', 'use', 'year', 'two', 'includ']\n",
      "Iteration: 00010\tll per word: -8.40584\tNum. of topics: 1755\n",
      "Iteration: 00020\tll per word: -8.12848\tNum. of topics: 2216\n",
      "Iteration: 00030\tll per word: -8.06211\tNum. of topics: 2350\n",
      "Iteration: 00040\tll per word: -8.02671\tNum. of topics: 2407\n",
      "Iteration: 00050\tll per word: -8.00521\tNum. of topics: 2452\n",
      "Iteration: 00060\tll per word: -7.98791\tNum. of topics: 2483\n",
      "Iteration: 00070\tll per word: -7.97682\tNum. of topics: 2478\n",
      "Iteration: 00080\tll per word: -7.96595\tNum. of topics: 2495\n",
      "Iteration: 00090\tll per word: -7.95793\tNum. of topics: 2504\n",
      "Iteration: 00100\tll per word: -7.95041\tNum. of topics: 2526\n",
      "Iteration: 00110\tll per word: -7.94312\tNum. of topics: 2546\n",
      "Iteration: 00120\tll per word: -7.93804\tNum. of topics: 2531\n",
      "Iteration: 00130\tll per word: -7.93332\tNum. of topics: 2550\n",
      "Iteration: 00140\tll per word: -7.92975\tNum. of topics: 2547\n",
      "Iteration: 00150\tll per word: -7.92511\tNum. of topics: 2571\n",
      "Iteration: 00160\tll per word: -7.92194\tNum. of topics: 2564\n",
      "Iteration: 00170\tll per word: -7.92018\tNum. of topics: 2564\n",
      "Iteration: 00180\tll per word: -7.91732\tNum. of topics: 2543\n",
      "Iteration: 00190\tll per word: -7.91498\tNum. of topics: 2569\n"
     ]
    }
   ],
   "source": [
    "def hlda_example(input_file, save_path):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    try:\n",
    "        cps = tp.utils.Corpus.load(input_file + '.cached.cps')\n",
    "    except IOError:\n",
    "        stemmer = PorterStemmer()\n",
    "        stops = set(stopwords.words('english'))\n",
    "        cps = tp.utils.Corpus(\n",
    "            tokenizer=tp.utils.SimpleTokenizer(stemmer=stemmer.stem), \n",
    "            stopwords=lambda x: len(x) <= 2 or x in stops\n",
    "        )\n",
    "        cps.process(open(input_file, encoding='utf-8'))\n",
    "        cps.save(input_file + '.cached.cps')\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    ridcs = np.random.permutation(len(cps))\n",
    "    test_idcs = ridcs[:20]\n",
    "    train_idcs = ridcs[20:]\n",
    "\n",
    "    test_cps = cps[test_idcs]\n",
    "    train_cps = cps[train_idcs]\n",
    "    \n",
    "    mdl = tp.HLDAModel(tw=tp.TermWeight.ONE, min_df=10, depth=4, rm_top=10, corpus=train_cps)\n",
    "    mdl.train(0)\n",
    "    print('Num docs:', len(mdl.docs), ', Vocab size:', len(mdl.used_vocabs), ', Num words:', mdl.num_words)\n",
    "    print('Removed top words:', mdl.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    for _ in range(0, 1000, 10):\n",
    "        mdl.train(7)\n",
    "        mdl.train(3, freeze_topics=True)\n",
    "        print('Iteration: {:05}\\tll per word: {:.5f}\\tNum. of topics: {}'.format(mdl.global_step, mdl.ll_per_word, mdl.live_k))\n",
    "\n",
    "    for _ in range(0, 100, 10):\n",
    "        mdl.train(10, freeze_topics=True)\n",
    "        print('Iteration: {:05}\\tll per word: {:.5f}\\tNum. of topics: {}'.format(mdl.global_step, mdl.ll_per_word, mdl.live_k))\n",
    "\n",
    "    mdl.summary()\n",
    "    print('Saving...', file=sys.stderr, flush=True)\n",
    "    mdl.save(save_path, True)\n",
    "\n",
    "    test_result_cps, ll = mdl.infer(test_cps)\n",
    "    for doc in test_result_cps:\n",
    "        print(doc.path, doc.get_words(top_n=10))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hlda_example('enwiki-16000.txt', 'test.hlda.tmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bbe71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
