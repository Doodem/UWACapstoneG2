{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a4b6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a332e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, contraction_mapping):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.contraction_mapping = contraction_mapping\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = self._to_lower(text)\n",
    "        text = self._expand_contractions(text)\n",
    "        text = self._remove_punctuation(text)\n",
    "        tokens = self._tokenize(text)\n",
    "        tokens = self._lemmatize_tokens(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def _to_lower(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def _expand_contractions(self, text):\n",
    "        for word, new_word in self.contraction_mapping.items():\n",
    "            text = text.replace(word, new_word)\n",
    "        return text\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def _lemmatize_tokens(self, tokens):\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13a9b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 Common Words:\n",
      "shall             1179313\n",
      "contractor         967271\n",
      "contract           675879\n",
      "general            556152\n",
      "provide            546267\n",
      "works              511310\n",
      "work               496976\n",
      "tender             492112\n",
      "site               478219\n",
      "services           449063\n",
      "required           440274\n",
      "requirements       401751\n",
      "principal          381564\n",
      "including          340190\n",
      "equipment          332259\n",
      "clause             324563\n",
      "building           317133\n",
      "10                 314273\n",
      "following          310297\n",
      "wa                 297076\n",
      "superintendent     283813\n",
      "date               280514\n",
      "drawings           267129\n",
      "construction       266744\n",
      "time               257678\n",
      "project            257517\n",
      "tenderer           255049\n",
      "information        253219\n",
      "mm                 252430\n",
      "page               251416\n",
      "materials          249724\n",
      "specification      239940\n",
      "authority          238911\n",
      "use                236412\n",
      "type               232379\n",
      "water              232223\n",
      "accordance         229518\n",
      "material           221564\n",
      "section            219398\n",
      "service            219136\n",
      "existing           218113\n",
      "details            216790\n",
      "control            213253\n",
      "installation       212794\n",
      "conditions         212622\n",
      "design             207782\n",
      "management         206655\n",
      "nzs                205601\n",
      "specified          205595\n",
      "steel              205180\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# list top 50 common words\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "directory_path = \"C:/Users/yfr/Downloads/Capstone/data/tender_raw\"\n",
    "os.chdir(directory_path)\n",
    "\n",
    "\n",
    "# read \"all_file_content.txt\" \n",
    "with open(\"all_file_content.txt\", \"r\", encoding=\"utf-8\") as input_file:\n",
    "    text_data = input_file.readlines()\n",
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=50,  \n",
    "    stop_words=\"english\",  \n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\", \n",
    ")\n",
    "\n",
    "\n",
    "word_counts = count_vectorizer.fit_transform(text_data)\n",
    "\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "word_counts_df = pd.DataFrame(data=word_counts.toarray(), columns=feature_names)\n",
    "\n",
    "\n",
    "top_50_common_words = word_counts_df.sum(axis=0).sort_values(ascending=False).head(50)\n",
    "print(\"Top 50 Common Words:\")\n",
    "print(top_50_common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31014f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common words\n",
    "\n",
    "processed_text_data = []\n",
    "\n",
    "for line in text_data:\n",
    "    words = line.split()\n",
    "    # filter words thats not in common_words\n",
    "    filtered_words = [word for word in words if word not in top_50_common_words.index]\n",
    "    processed_line = \" \".join(filtered_words)\n",
    "    processed_text_data.append(processed_line)\n",
    "\n",
    "# save processed text in \"all_file_content.txt\"\n",
    "with open(\"all_file_content_processed.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.writelines(processed_text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca727d70",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3664\\4031733596.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtfidf_matrix_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer_processed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_data_processed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mfeature_names_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer_processed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2075\u001b[0m         \"\"\"\n\u001b[0;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TF-IDF after common words removed\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open(\"all_file_content_processed.txt\", \"r\", encoding=\"utf-8\") as input_file:\n",
    "    text_data_processed = input_file.readlines()\n",
    "\n",
    "\n",
    "tfidf_vectorizer_processed = TfidfVectorizer(\n",
    "    max_features=50,  \n",
    "    stop_words=None,  # no stop words needed, common words alr removed\n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    ")\n",
    "\n",
    "\n",
    "tfidf_matrix_processed = tfidf_vectorizer_processed.fit_transform(text_data_processed)\n",
    "\n",
    "feature_names_processed = tfidf_vectorizer_processed.get_feature_names_out()\n",
    "\n",
    "tfidf_df_processed = pd.DataFrame(data=tfidf_matrix_processed.toarray(), columns=feature_names_processed)\n",
    "\n",
    "top_50_tfidf_words_processed = tfidf_df_processed.sum(axis=0).sort_values(ascending=False).head(50)\n",
    "print(\"Top 50 Words by TF-IDF in Processed Text:\")\n",
    "print(top_50_tfidf_words_processed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
