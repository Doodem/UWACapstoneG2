{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd11634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tomotopy\n",
      "  Downloading tomotopy-0.12.5-cp39-cp39-win_amd64.whl (5.7 MB)\n",
      "     ---------------------------------------- 5.7/5.7 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from tomotopy) (1.21.5)\n",
      "Installing collected packages: tomotopy\n",
      "Successfully installed tomotopy-0.12.5\n"
     ]
    }
   ],
   "source": [
    "!pip install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9458245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\yfr\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a598da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tomotopy as tp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1771fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file with 'contract title' and 'description' columns\n",
    "file_path = 'TendersWA.xlsx'\n",
    "df = pd.read_excel(file_path, usecols=['Contract Title', 'Description'])\n",
    "df['Combined'] = df['Contract Title'] + ' ' + df['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f577619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92f7fff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, contraction_mapping):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.contraction_mapping = contraction_mapping\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = self._to_lower(text)\n",
    "        text = self._expand_contractions(text)\n",
    "        text = self._remove_punctuation(text)\n",
    "        tokens = self._tokenize(text)\n",
    "        tokens = self._remove_stop_words(tokens)\n",
    "        tokens = self._lemmatize_tokens(tokens)\n",
    "        return tokens\n",
    "    def preprocess_text(self, text):\n",
    "        if isinstance(text, str):  \n",
    "            text = self._to_lower(text)\n",
    "            text = self._expand_contractions(text)\n",
    "            text = self._remove_punctuation(text)\n",
    "            tokens = self._tokenize(text)\n",
    "            tokens = self._remove_stop_words(tokens)\n",
    "            tokens = self._lemmatize_tokens(tokens)\n",
    "            return tokens\n",
    "        else:\n",
    "            return [] \n",
    "\n",
    "    def _to_lower(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def _expand_contractions(self, text):\n",
    "        for word, new_word in self.contraction_mapping.items():\n",
    "            text = text.replace(word, new_word)\n",
    "        return text\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def _lemmatize_tokens(self, tokens):\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    def _remove_stop_words(self, tokens):\n",
    "        return [token for token in tokens if token not in self.stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b3c2020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Combined  \\\n",
      "0      Supply and Delivery of Aspen Bedding Material ...   \n",
      "1      Bar Consultancy and Staffing AGWA Rooftop Bar ...   \n",
      "2      Investment Services for Art Gallery WA Through...   \n",
      "3      Provision of Audience Research Provision of Au...   \n",
      "4      Consultancy Services for HR Support Provision ...   \n",
      "...                                                  ...   \n",
      "26140  Provision of Electricity Services for WorkCove...   \n",
      "26141  Redevelop WorkCover WA Conference Website in W...   \n",
      "26142  Core Business Systems Technology Refresh FY201...   \n",
      "26143  Core Business Systems Technology Refresh FY201...   \n",
      "26144  Digital First Conciliation and Arbitration Ser...   \n",
      "\n",
      "                                               Processed  \n",
      "0      [supply, delivery, aspen, bedding, material, c...  \n",
      "1      [bar, consultancy, staffing, agwa, rooftop, ba...  \n",
      "2      [investment, service, art, gallery, wa, reques...  \n",
      "3      [provision, audience, research, provision, aud...  \n",
      "4      [consultancy, service, hr, support, provision,...  \n",
      "...                                                  ...  \n",
      "26140  [provision, electricity, service, workcover, w...  \n",
      "26141  [redevelop, workcover, wa, conference, website...  \n",
      "26142  [core, business, system, technology, refresh, ...  \n",
      "26143  [core, business, system, technology, refresh, ...  \n",
      "26144  [digital, first, conciliation, arbitration, se...  \n",
      "\n",
      "[26145 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "processor = TextProcessor(contraction_mapping={})\n",
    "\n",
    "df['Processed'] = df['Combined'].apply(processor.preprocess_text)\n",
    "\n",
    "print(df[['Combined', 'Processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb8e4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an H-LDA model\n",
    "hlda_model = tp.HLDAModel(tw=tp.TermWeight.ONE, min_df=5)\n",
    "\n",
    "# Add preprocessed documents to the model (assuming 'Processed' column contains preprocessed text)\n",
    "for index, row in df.iterrows():\n",
    "    processed_text = row['Processed']  # Assuming 'Processed' column contains preprocessed text\n",
    "    hlda_model.add_doc(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50d26318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Perplexity = inf\n",
      "Iteration 2: Perplexity = inf\n",
      "Iteration 3: Perplexity = inf\n",
      "Iteration 4: Perplexity = inf\n",
      "Iteration 5: Perplexity = inf\n",
      "Iteration 6: Perplexity = inf\n",
      "Iteration 7: Perplexity = inf\n",
      "Iteration 8: Perplexity = inf\n",
      "Iteration 9: Perplexity = inf\n",
      "Iteration 10: Perplexity = inf\n",
      "Iteration 11: Perplexity = inf\n",
      "Iteration 12: Perplexity = inf\n",
      "Iteration 13: Perplexity = inf"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15196\\3451906243.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mhlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Iteration {i + 1}: Perplexity = {hlda_model.perplexity}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    553\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_schedule_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36m_schedule_flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_later\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_interval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_schedule_in_thread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    616\u001b[0m                 )\n\u001b[0;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     def send_multipart(\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the H-LDA model\n",
    "for i in range(20):\n",
    "    hlda_model.train(1)\n",
    "    print(f\"Iteration {i + 1}: Perplexity = {hlda_model.perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee1ee964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0:\n",
      "Topic 0 (Parent: -1): []\n",
      "Topic 8 (Parent: 0): []\n",
      "Topic 9 (Parent: 0): []\n",
      "Topic 10 (Parent: 0): []\n",
      "Topic 11 (Parent: 0): []\n",
      "Topic 12 (Parent: 0): []\n",
      "Topic 13 (Parent: 0): []\n",
      "Topic 14 (Parent: 0): []\n",
      "Topic 15 (Parent: 0): []\n",
      "Topic 16 (Parent: 0): []\n",
      "Topic 17 (Parent: 0): []\n",
      "Topic 18 (Parent: 0): []\n",
      "Topic 19 (Parent: 0): []\n",
      "Topic 20 (Parent: 0): []\n",
      "Topic 21 (Parent: 0): []\n",
      "Topic 22 (Parent: 0): []\n",
      "Level 1:\n",
      "Topic 0 (Parent: -1): [(' ', 0.1421549916267395)]\n",
      "Topic 8 (Parent: 0): [(' ', 0.14907360076904297)]\n",
      "Topic 9 (Parent: 0): [(' ', 0.14046309888362885)]\n",
      "Topic 10 (Parent: 0): [(' ', 0.10922178626060486)]\n",
      "Topic 11 (Parent: 0): [(' ', 0.14229562878608704)]\n",
      "Topic 12 (Parent: 0): [('n', 0.15040075778961182)]\n",
      "Topic 13 (Parent: 0): [('x', 0.5293495655059814)]\n",
      "Topic 14 (Parent: 0): [('*', 0.2170143723487854)]\n",
      "Topic 15 (Parent: 0): [(' ', 0.12031947821378708)]\n",
      "Topic 16 (Parent: 0): [(' ', 0.1409442275762558)]\n",
      "Topic 17 (Parent: 0): [('\\xa0', 0.3260754644870758)]\n",
      "Topic 18 (Parent: 0): [('2', 0.19928602874279022)]\n",
      "Topic 19 (Parent: 0): [(' ', 0.13281500339508057)]\n",
      "Topic 20 (Parent: 0): [(' ', 0.13252787292003632)]\n",
      "Topic 21 (Parent: 0): [('*', 0.7666292786598206)]\n",
      "Topic 22 (Parent: 0): [('X', 0.3106119632720947)]\n"
     ]
    }
   ],
   "source": [
    "# Print topics and their hierarchies\n",
    "for level in range(hlda_model.depth):\n",
    "    print(f\"Level {level}:\")\n",
    "    for topic_id in range(hlda_model.live_k):\n",
    "        if hlda_model.is_live_topic(topic_id):\n",
    "            topic_words = hlda_model.get_topic_words(topic_id, level)\n",
    "            parent_topic = hlda_model.parent_topic(topic_id)\n",
    "            print(f\"Topic {topic_id} (Parent: {parent_topic}): {topic_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f1c8ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 (Parent: None): []\n",
      "  Topic 8 (Parent: 0): [(' ', 0.14907360076904297)]\n",
      "  Topic 9 (Parent: 0): [(' ', 0.14046309888362885)]\n",
      "  Topic 10 (Parent: 0): [(' ', 0.10922178626060486)]\n",
      "  Topic 11 (Parent: 0): [(' ', 0.14229562878608704)]\n",
      "  Topic 12 (Parent: 0): [('n', 0.15040075778961182)]\n",
      "  Topic 13 (Parent: 0): [('x', 0.5293495655059814)]\n",
      "  Topic 14 (Parent: 0): [('*', 0.2170143723487854)]\n",
      "  Topic 15 (Parent: 0): [(' ', 0.12031947821378708)]\n",
      "  Topic 16 (Parent: 0): [(' ', 0.1409442275762558)]\n",
      "  Topic 17 (Parent: 0): [('\\xa0', 0.3260754644870758)]\n",
      "  Topic 18 (Parent: 0): [('2', 0.19928602874279022)]\n",
      "  Topic 19 (Parent: 0): [(' ', 0.13281500339508057)]\n",
      "  Topic 20 (Parent: 0): [(' ', 0.13252787292003632)]\n",
      "  Topic 21 (Parent: 0): [('*', 0.7666292786598206)]\n",
      "  Topic 22 (Parent: 0): [('X', 0.3106119632720947)]\n"
     ]
    }
   ],
   "source": [
    "def visualize_hierarchy(model, level, parent=None, indent=\"\"):\n",
    "    for topic_id in range(model.live_k):\n",
    "        if model.is_live_topic(topic_id) and model.level(topic_id) == level:\n",
    "            topic_words = model.get_topic_words(topic_id, level)\n",
    "            print(f\"{indent}Topic {topic_id} (Parent: {parent}): {topic_words}\")\n",
    "            visualize_hierarchy(model, level + 1, parent=topic_id, indent=indent + \"  \")\n",
    "\n",
    "# Visualize the hierarchy starting from level 0\n",
    "visualize_hierarchy(hlda_model, level=0)\n",
    "\n",
    "# Show the plot (you can customize this part for your specific visualization needs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ee94b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
