{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd11634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tomotopy\n",
      "  Downloading tomotopy-0.12.5-cp39-cp39-win_amd64.whl (5.7 MB)\n",
      "     ---------------------------------------- 5.7/5.7 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from tomotopy) (1.21.5)\n",
      "Installing collected packages: tomotopy\n",
      "Successfully installed tomotopy-0.12.5\n"
     ]
    }
   ],
   "source": [
    "!pip install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9458245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\yfr\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\yfr\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a598da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tomotopy as tp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1771fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file with 'contract title' and 'description' columns\n",
    "file_path = 'TendersWA.xlsx'\n",
    "df = pd.read_excel(file_path, usecols=['Contract Title', 'Description'])\n",
    "df['Combined'] = df['Contract Title'] + ' ' + df['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f577619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f7fff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, contraction_mapping):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.contraction_mapping = contraction_mapping\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if isinstance(text, str):  \n",
    "            text = self._to_lower(text)\n",
    "            text = self._expand_contractions(text)\n",
    "            text = self._remove_punctuation(text)\n",
    "            text = self._remove_stop_words(text)\n",
    "            return text\n",
    "        else:\n",
    "            return ''  # 返回空字符串而不是空列表\n",
    "\n",
    "    def _to_lower(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def _expand_contractions(self, text):\n",
    "        for word, new_word in self.contraction_mapping.items():\n",
    "            text = text.replace(word, new_word)\n",
    "        return text\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def _remove_stop_words(self, text):\n",
    "        return ' '.join([token for token in text.split() if token not in self.stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3c2020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Combined  \\\n",
      "0      Supply and Delivery of Aspen Bedding Material ...   \n",
      "1      Bar Consultancy and Staffing AGWA Rooftop Bar ...   \n",
      "2      Investment Services for Art Gallery WA Through...   \n",
      "3      Provision of Audience Research Provision of Au...   \n",
      "4      Consultancy Services for HR Support Provision ...   \n",
      "...                                                  ...   \n",
      "26140  Provision of Electricity Services for WorkCove...   \n",
      "26141  Redevelop WorkCover WA Conference Website in W...   \n",
      "26142  Core Business Systems Technology Refresh FY201...   \n",
      "26143  Core Business Systems Technology Refresh FY201...   \n",
      "26144  Digital First Conciliation and Arbitration Ser...   \n",
      "\n",
      "                                               Processed  \n",
      "0      supply delivery aspen bedding material custome...  \n",
      "1      bar consultancy staffing agwa rooftop bar agwa...  \n",
      "2      investment services art gallery wa request art...  \n",
      "3      provision audience research provision audience...  \n",
      "4      consultancy services hr support provision spec...  \n",
      "...                                                  ...  \n",
      "26140  provision electricity services workcover wa pr...  \n",
      "26141  redevelop workcover wa conference website word...  \n",
      "26142  core business systems technology refresh fy201...  \n",
      "26143  core business systems technology refresh fy201...  \n",
      "26144  digital first conciliation arbitration service...  \n",
      "\n",
      "[26145 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "processor = TextProcessor(contraction_mapping={})\n",
    "\n",
    "df['Processed'] = df['Combined'].apply(processor.preprocess_text)\n",
    "\n",
    "print(df[['Combined', 'Processed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb8e4e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yfr\\AppData\\Local\\Temp\\ipykernel_8288\\1966725409.py:7: RuntimeWarning: `words` should be an iterable of str.\n",
      "  hlda_model.add_doc(processed_text)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an H-LDA model\n",
    "hlda_model = tp.HLDAModel(tw=tp.TermWeight.ONE, min_df=5)\n",
    "\n",
    "# Add preprocessed documents to the model (assuming 'Processed' column contains preprocessed text)\n",
    "for index, row in df.iterrows():\n",
    "    processed_text = row['Processed']\n",
    "    hlda_model.add_doc(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d26318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the H-LDA model\n",
    "for i in range(100):\n",
    "    hlda_model.train(1)\n",
    "    print(f\"Iteration {i + 1}: Perplexity = {hlda_model.perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee1ee964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0:\n",
      "Topic 0 (Parent: -1): []\n",
      "Level 1:\n",
      "Topic 0 (Parent: -1): [(' ', 0.12014854699373245)]\n"
     ]
    }
   ],
   "source": [
    "# Print topics and their hierarchies\n",
    "for level in range(hlda_model.depth):\n",
    "    print(f\"Level {level}:\")\n",
    "    for topic_id in range(hlda_model.live_k):\n",
    "        if hlda_model.is_live_topic(topic_id):\n",
    "            topic_words = hlda_model.get_topic_words(topic_id, level)\n",
    "            parent_topic = hlda_model.parent_topic(topic_id)\n",
    "            print(f\"Topic {topic_id} (Parent: {parent_topic}): {topic_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
