{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# Data Processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMbbym3_28dT"
      },
      "source": [
        "## 1.1 Package and Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFqPM9rp13M9"
      },
      "source": [
        "Prior to starting any of the data loading and processing, all nessessary packages should be loaded. The code cell below will load them all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMlegiRG12p3",
        "outputId": "4bdd1405-c6ee-4b32-c480-19f1e9d6f022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import math\n",
        "import gensim.downloader as api\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from torch.cuda.amp import GradScaler\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_recall_curve, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_5mC5EJ2xrL"
      },
      "source": [
        "Set the torch start states to a fixed value so that the results can be replicated when the code has be run again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rArM8dU024z3"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(27)\n",
        "torch.cuda.manual_seed_all(27)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0I_IIBr1IcA"
      },
      "source": [
        "Set the path to the location of the datasets and load them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "qvff21Hv8zjk",
        "outputId": "2a91fca3-0506-47f8-8e94-e3b19cbb106b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-67c899944e00>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Visualise the data to check if it loaded correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ],
      "source": [
        "#drive.mount('/content/drive',force_remount=False)\n",
        "\n",
        "# Load the datasets\n",
        "\n",
        "# Visualise the data to check if it loaded correctly\n",
        "#train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxqDgnzW3AfX"
      },
      "source": [
        "## 1.2 Data Preperation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Rp9uSt4BVf"
      },
      "source": [
        "Data will be processed from a raw string format into a NLP use format. The 'Textprocessor' class is what does this, it has various steps which perform conversion to lowercase, expanding contradictions, removing punctuations, tokenisation of text, and lemmatisation of token which reduces them to their root form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vBGNcqaK4W45"
      },
      "outputs": [],
      "source": [
        "class Textprocessor:\n",
        "    def __init__(self, contraction_mapping):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.contraction_mapping = contraction_mapping\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = self._to_lower(text)\n",
        "        text = self._expand_contractions(text)\n",
        "        text = self._remove_punctuation(text)\n",
        "        tokens = self._tokenize(text)\n",
        "        tokens = self._lemmatize_tokens(tokens)\n",
        "        return tokens\n",
        "\n",
        "    def _to_lower(self, text):\n",
        "        return text.lower()\n",
        "\n",
        "    def _expand_contractions(self, text):\n",
        "        for word, new_word in self.contraction_mapping.items():\n",
        "            text = text.replace(word, new_word)\n",
        "        return text\n",
        "\n",
        "    def _remove_punctuation(self, text):\n",
        "        return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def _lemmatize_tokens(self, tokens):\n",
        "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}