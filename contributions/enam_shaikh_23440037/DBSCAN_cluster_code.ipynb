{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca03ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "file_path = 'merged_file.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Specify the column containing the embeddings (replace 'Embeddings_Column' with the actual column name)\n",
    "embeddings_column_name = 'Embeddings'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d36037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse embeddings\n",
    "def parse_embeddings(embeddings_str):\n",
    "    # Split the string by comma and remove any leading/trailing whitespace\n",
    "    values = embeddings_str.strip('[]').split(',')\n",
    "    #print(len(values))\n",
    "    # Convert the values to float and create a NumPy array\n",
    "    return np.array([float(val) for val in values]).round(10)\n",
    "\n",
    "# Apply the parse_embeddings function to the 'Embeddings' column\n",
    "df['Embeddings'] = df['Embeddings'].apply(parse_embeddings)\n",
    "\n",
    "# Create a new DataFrame with 768 columns, each containing one embedding value\n",
    "embedding_df = pd.DataFrame(df['Embeddings'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabea1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = embedding_df.shape[1] // 2\n",
    "\n",
    "# Keep only the first 'columns_to_keep' columns and drop the rest\n",
    "embedding_df = embedding_df.iloc[:, :columns_to_keep]\n",
    "\n",
    "# Remove any rows with NaN values\n",
    "embedding_df.dropna(inplace=True)\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "embedding_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64629972",
   "metadata": {},
   "source": [
    "Scale the data before appying DBSCAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(embedding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score,silhouette_samples\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18404d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "#file path for the output text file\n",
    "output_file_path = 'output_messages.txt'\n",
    "\n",
    "# Open the file for writing\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "\n",
    "    cluster = []\n",
    "\n",
    "    for k in range(1, 20):\n",
    "\n",
    "        model = DBSCAN(eps=k * 0.1)\n",
    "\n",
    "        cluster_labels = model.fit_predict(embedding_df)\n",
    "\n",
    "        unique_labels, label_counts = np.unique(cluster_labels, return_counts=True)\n",
    "\n",
    "        \n",
    "        for label, count in zip(unique_labels, label_counts):\n",
    "            message = f\"Cluster {label}: {count} instances\"\n",
    "            output_file.write(message + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b117fa",
   "metadata": {},
   "source": [
    "Below code is to create a dataframe for each epsilon values and how many clusters were created and how many tenders are available in each clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to collect cluster information\n",
    "cluster_info = []\n",
    "\n",
    "# Define the range of epsilon values you want to test\n",
    "epsilon_range = np.arange(0.1, 2.0, 0.1)\n",
    "\n",
    "for epsilon in epsilon_range:\n",
    "    model = DBSCAN(eps=epsilon)\n",
    "    cluster_labels = model.fit_predict(embedding_df)\n",
    "    unique_labels, label_counts = np.unique(cluster_labels, return_counts=True)\n",
    "\n",
    "    # Create a dictionary to store cluster information\n",
    "    cluster_data = {\n",
    "        \"Epsilon\": epsilon,\n",
    "        \"Num_Clusters\": len(unique_labels),\n",
    "        \"Cluster_Info\": {label: count for label, count in zip(unique_labels, label_counts)}\n",
    "    }\n",
    "\n",
    "    # Append the cluster information to the list\n",
    "    cluster_info.append(cluster_data)\n",
    "\n",
    "# Create a DataFrame from the list of cluster information\n",
    "dbscan_cluster_df = pd.DataFrame(cluster_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d6b85",
   "metadata": {},
   "source": [
    "To create a barplot which shows top 10 clusters which has most tenders for each epsilon values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for index, row in dbscan_cluster_df.iterrows():\n",
    "    epsilon = row['Epsilon']\n",
    "    cluster_info = row['Cluster_Info']\n",
    "\n",
    "    # Sort the cluster_info by the number of instances in descending order\n",
    "    sorted_clusters = sorted(cluster_info.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Get the top 10 clusters and their counts\n",
    "    top_clusters = [item[0] for item in sorted_clusters[:10]]\n",
    "    top_counts = [item[1] for item in sorted_clusters[:10]]\n",
    "    cluster_labels = [f'Cluster {cluster}' for cluster in top_clusters]\n",
    "\n",
    "    # Create a bar plot for the top clusters\n",
    "    plt.barh(cluster_labels, top_counts)\n",
    "    plt.xlabel('Number of Tenders')\n",
    "    plt.ylabel('Cluster Label')\n",
    "    plt.title(f'Top 10 Clusters for Epsilon = {epsilon}')\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis to show the highest count at the top\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eaf2eb",
   "metadata": {},
   "source": [
    "To calculate the silhouette_score for DBSCAN model and plot the scores. ( Might not be useful )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the range of epsilon values\n",
    "epsilon_range = np.linspace(1.0, 2.0, num=10)  # Adjust the range as needed\n",
    "\n",
    "# Create an empty list to store silhouette scores for each epsilon\n",
    "silhouette_scores = []\n",
    "\n",
    "# Iterate over the range of epsilon values\n",
    "for epsilon in epsilon_range:\n",
    "    # Create and fit a DBSCAN model with the current epsilon\n",
    "    dbscan = DBSCAN(eps=epsilon, min_samples=5)\n",
    "    cluster_labels = dbscan.fit_predict(embedding_df)\n",
    "    \n",
    "    # Calculate the silhouette score\n",
    "    score = silhouette_score(embedding_df, cluster_labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot the silhouette scores for different epsilon values\n",
    "plt.plot(epsilon_range, silhouette_scores, 'o--')\n",
    "plt.xlabel(\"Epsilon Value\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs. Epsilon Value (DBSCAN)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba5634",
   "metadata": {},
   "source": [
    "# Kmeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.cluster._kmeans\")\n",
    "\n",
    "ssd = []\n",
    "\n",
    "for k in range(25,50):\n",
    "    \n",
    "    model = KMeans(n_clusters=k,random_state=5508)\n",
    "    \n",
    "    \n",
    "    model.fit(embedding_df)\n",
    "    \n",
    "    #Sum of squared distances of samples to their closest cluster center.\n",
    "    ssd.append(model.inertia_)\n",
    "plt.plot(range(150,200),ssd,'o--')\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\" Sum of Squared Distances\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa358aba",
   "metadata": {},
   "source": [
    "When applying the elbow method to determine the optimal value of K, it is challenging to identify clear elbow points in the sum of squared distances (SSD) plot. In such cases, an alternative technique like silhouette analysis can be employed to make a more informed decision about the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67943d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# define range of K values\n",
    "k_range = range(2, 25)\n",
    "\n",
    "# empty list to store silhouette scores for each K\n",
    "silhouette_scores = []\n",
    "\n",
    "# loop over K values\n",
    "for k in k_range:\n",
    "    # fit K-means model\n",
    "    kmeans = KMeans(n_clusters=k, random_state=5508).fit(embedding_df)\n",
    "    # compute silhouette score\n",
    "    score = silhouette_score(embedding_df, kmeans.labels_, metric='euclidean')\n",
    "    silhouette_scores.append(score)\n",
    "plt.plot(k_range,silhouette_scores,'o--')\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\" Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs. Number of Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1579b",
   "metadata": {},
   "source": [
    "So far we are considering 415 tenders , open individually looking at the tenders, i feel like more than 10 clusters should be performed.\n",
    "\n",
    "When the K value increases K value doesn't change much which indicates that additional clusters beyond K=3 do not improve the clustering quality significantly. The clusters may continue to have similar patterns of separation and cohesion, leading to similar average silhouette widths.\n",
    "\n",
    "By looking at this it seems that k= 2 could be an optimal value, let's look at the silhouette diagram and see how individual clusters performs for each K value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using silhouette analysis to find optimal K value\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random state\n",
    "np.random.seed(5508)\n",
    "\n",
    "# Define the range of K values\n",
    "K_values = range(6, 19)\n",
    "\n",
    "# Initialize a list to store the silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# Create a subplot for each K value\n",
    "fig, axs = plt.subplots(len(K_values), 1, figsize=(8, 6 * len(K_values)))\n",
    "\n",
    "# Iterate over the range of K values\n",
    "for i, K in enumerate(K_values):\n",
    "    # Fit the K-means model\n",
    "    kmeans = KMeans(n_clusters=K, random_state=5508)\n",
    "    kmeans.fit(embedding_df)\n",
    "\n",
    "    # Obtain the cluster labels\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Compute the silhouette score\n",
    "    silhouette_avg = silhouette_score(embedding_df, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot the silhouette diagram\n",
    "    ax = axs[i]\n",
    "    y_lower = 10\n",
    "\n",
    "    for j in range(K):\n",
    "        # Collect silhouette scores for samples in the current cluster\n",
    "        cluster_silhouette_scores = silhouette_samples(embedding_df, labels)[labels == j]\n",
    "        cluster_silhouette_scores.sort()\n",
    "\n",
    "        size_cluster_j = cluster_silhouette_scores.shape[0]\n",
    "        y_upper = y_lower + size_cluster_j\n",
    "\n",
    "        color = plt.cm.get_cmap(\"Spectral\")(j / K)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_scores, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the cluster with its silhouette score\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_j, str(j))\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "    ax.set_title(f\"K={K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering\n",
    "kmeans = KMeans(n_clusters=10, random_state=5508)\n",
    "cluster_labels = kmeans.fit_predict(embedding_df)\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "df['Cluster_Label'] = cluster_labels\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "updated_file_path = 'updated_excel_file.xlsx'\n",
    "df.to_excel(updated_file_path, index=False)\n",
    "\n",
    "# Print the cluster labels\n",
    "print(\"Cluster labels:\")\n",
    "print(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original Excel file into a DataFrame\n",
    "file_path = 'updated_excel_file.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Specify the columns you want to keep (replace 'Column1' and 'Column2' with the actual column names)\n",
    "columns_to_keep = ['Reference Number', 'Client Agency','Developing Agency Parent','Contract Title','Description','UNSPSC Title','Cluster_Label']\n",
    "\n",
    "# Create a new DataFrame with only the specified columns\n",
    "new_df = df[columns_to_keep]\n",
    "\n",
    "# Save the new DataFrame to a new Excel file\n",
    "new_file_path = 'cluster_file.xlsx'\n",
    "new_df.to_excel(new_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77935bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e81df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
