{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5616c483-731c-4ad8-a50e-3b0421d16b05",
   "metadata": {},
   "source": [
    "# Load Summarisation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5adc3f7-3972-43a5-83f1-ac5e0a25b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class Summariser:\n",
    "\n",
    "    def __init__(self, max_length, min_length, chunk_size, type=False, do_sample=False):\n",
    "        \"\"\" Summarises text inputs \"\"\"\n",
    "\n",
    "        if type:\n",
    "            trained = type\n",
    "        else:\n",
    "            trained = 'xsum'\n",
    "        self.summarizer = pipeline('summarization', model=f'facebook/bart-large-{trained}')\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.do_sample = do_sample\n",
    "        self.chunk_size = chunk_size\n",
    "        self.summarised_docs = {}\n",
    "\n",
    "    def summarise_docs(self, docs):\n",
    "        \"\"\" \n",
    "        max_length (int): Maximum length of the generated summary.\n",
    "        min_length (int): Minimum length of the generated summary.\n",
    "        do_sample (bool): Whether to use greedy sampling when generating summaries.\n",
    "        chunk_size (int): Max size of chunk if need to summarise chunks seperately becuase input too large.\n",
    "        \n",
    "        Returns: dict of summaries per ref\n",
    "        \"\"\"\n",
    "        \n",
    "        for ref, keys in docs.items():\n",
    "            # combine text\n",
    "            text = self.combine(keys)\n",
    "            \n",
    "            # if extra information exists\n",
    "            if len(keys) > 2:\n",
    "            \n",
    "                # If chunking is enabled and text is too long\n",
    "                if self.chunk_size and len(text) > self.chunk_size:\n",
    "                    chunks = self.chunk_text(text, self.chunk_size)\n",
    "                    summary_text = \"\"\n",
    "                    for chunk in chunks:\n",
    "                        try:\n",
    "                            summary = self.summarizer(chunk, max_length=self.max_length, min_length=self.min_length, do_sample=self.do_sample)\n",
    "                            summary_text += summary[0]['summary_text'] + \"\\n\"\n",
    "                        except:\n",
    "                            print(f\"Chunked document for '{ref}' is too long\")\n",
    "                            continue\n",
    "                    self.summarised_docs[ref] = summary_text.strip()\n",
    "                    print(f\"{ref} summarised in chunks\")\n",
    "                    \n",
    "                # summarise as normal if extra information not long\n",
    "                else:\n",
    "                    try:\n",
    "                        summary = self.summarizer(text, max_length=self.max_length, min_length=self.min_length, do_sample=self.do_sample)\n",
    "                        self.summarised_docs[ref] = summary[0]['summary_text']\n",
    "                        print(f\"{ref} summarised\")\n",
    "                    except:\n",
    "                        print(f\"Error occured while processing {ref}!!!\")\n",
    "                        continue\n",
    "            # don't summarise title and short desc\n",
    "            else:\n",
    "                self.summarised_docs[ref] = text\n",
    "                print(f\"{ref} not summarised\")\n",
    "        \n",
    "        return self.summarised_docs\n",
    "        \n",
    "    def summary(self, ref):\n",
    "        \"\"\" Return summaries from input ref \"\"\"\n",
    "        return print(self.summarised_docs[ref])\n",
    "    \n",
    "    def combine(self, keys):\n",
    "        text = \"\"\n",
    "        for key, text in keys.items():\n",
    "            if text is not None:\n",
    "                text += text\n",
    "        return text\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size):\n",
    "        \"\"\" Split long text into chunks of specified size to summarise seperately \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            start = end\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82c73c-d1e6-4cc0-b995-f547b4af22a4",
   "metadata": {},
   "source": [
    "# Read Pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8923f65-4b19-402a-988a-b2ee81741f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def pickler(path):\n",
    "    pickles_read = {}\n",
    "    pickles_unread = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.pickle'):\n",
    "            file_path = os.path.join(path, file)\n",
    "            ref = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            try:\n",
    "                with open(file_path, \"rb\") as data:\n",
    "                    pickles_read[ref] = pickle.load(data)\n",
    "            except:\n",
    "                pickles_unread.append(file)\n",
    "    \n",
    "    return pickles_read, pickles_unread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21588d-1a1c-4e25-904a-ae068af79dbd",
   "metadata": {},
   "source": [
    "# Summarise Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e27dc-0aae-40f8-b2c2-086bb2057af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "pickle_path = \"C:/Users/Mitch/pickles/\"\n",
    "pickles, empty_pickles = pickler(pickle_path)\n",
    "\n",
    "random_pickle_keys = random.sample(list(pickles.keys()), 100)\n",
    "random_pickles = {key: pickles[key] for key in random_pickle_keys}\n",
    "\n",
    "summariser = Summariser(max_length=24, min_length=10, chunk_size=2000, do_sample=False) # can add type='cnn' to change what model trained on\n",
    "summarised_docs = summariser.summarise_docs(random_pickles)\n",
    "\n",
    "for ref, summary in summarised_docs.items():\n",
    "    print(f\"Summary for {ref}:\\n{summary}\")\n",
    "    print(\"<============================>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be333d1-00bd-4d07-876c-9f86c486dba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
