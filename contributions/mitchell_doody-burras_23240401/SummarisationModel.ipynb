{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5616c483-731c-4ad8-a50e-3b0421d16b05",
   "metadata": {},
   "source": [
    "# Load Summarisation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa73c6f-9895-4451-a685-5ad602a1aa3b",
   "metadata": {},
   "source": [
    "# Method 1: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5adc3f7-3972-43a5-83f1-ac5e0a25b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class Summariser:\n",
    "\n",
    "    def __init__(self, max_length, min_length, chunk_size, type=False, do_sample=False):\n",
    "        \"\"\" Summarises text inputs \"\"\"\n",
    "\n",
    "        if type:\n",
    "            trained = type\n",
    "        else:\n",
    "            trained = 'xsum'\n",
    "        self.summarizer = pipeline('summarization', model=f'facebook/bart-large-{trained}')\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.do_sample = do_sample\n",
    "        self.chunk_size = chunk_size\n",
    "        self.summarised_docs = {}\n",
    "\n",
    "    def summarise_docs(self, docs):\n",
    "        \"\"\" \n",
    "        max_length (int): Maximum length of the generated summary.\n",
    "        min_length (int): Minimum length of the generated summary.\n",
    "        do_sample (bool): Whether to use greedy sampling when generating summaries.\n",
    "        chunk_size (int): Max size of chunk if need to summarise chunks seperately becuase input too large.\n",
    "        \n",
    "        Returns: dict of summaries per ref\n",
    "        \"\"\"\n",
    "        \n",
    "        for ref, keys in docs.items():\n",
    "            # combine text\n",
    "            text = self.combine(keys)\n",
    "            \n",
    "            # if extra information exists\n",
    "            if len(keys) > 2:\n",
    "            \n",
    "                # If chunking is enabled and text is too long\n",
    "                if self.chunk_size and len(text) > self.chunk_size:\n",
    "                    chunks = self.chunk_text(text, self.chunk_size)\n",
    "                    summary_text = \"\"\n",
    "                    for chunk in chunks:\n",
    "                        try:\n",
    "                            summary = self.summarizer(chunk, max_length=self.max_length, min_length=self.min_length, do_sample=self.do_sample)\n",
    "                            summary_text += summary[0]['summary_text'] + \"\\n\"\n",
    "                        except:\n",
    "                            print(f\"Chunked document for '{ref}' is too long\")\n",
    "                            continue\n",
    "                    self.summarised_docs[ref] = summary_text.strip()\n",
    "                    print(f\"{ref} summarised in chunks\")\n",
    "                    \n",
    "                # summarise as normal if extra information not long\n",
    "                else:\n",
    "                    try:\n",
    "                        summary = self.summarizer(text, max_length=self.max_length, min_length=self.min_length, do_sample=self.do_sample)\n",
    "                        self.summarised_docs[ref] = summary[0]['summary_text']\n",
    "                        print(f\"{ref} summarised\")\n",
    "                    except:\n",
    "                        print(f\"Error occured while processing {ref}!!!\")\n",
    "                        continue\n",
    "            # don't summarise title and short desc\n",
    "            else:\n",
    "                self.summarised_docs[ref] = text\n",
    "                print(f\"{ref} not summarised\")\n",
    "        \n",
    "        return self.summarised_docs\n",
    "        \n",
    "    def summary(self, ref):\n",
    "        \"\"\" Return summaries from input ref \"\"\"\n",
    "        return print(self.summarised_docs[ref])\n",
    "    \n",
    "    def combine(self, keys):\n",
    "        text = \"\"\n",
    "        for key, text in keys.items():\n",
    "            if text is not None:\n",
    "                text += text\n",
    "        return text\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size):\n",
    "        \"\"\" Split long text into chunks of specified size to summarise seperately \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            start = end\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe46053-7b9a-4e76-b777-7f743aba4823",
   "metadata": {},
   "source": [
    "# Method 2: Extract relevant text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6be3c-f1dd-41a6-8d70-a36f6a714cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "class Summariser:\n",
    "\n",
    "    def __init__(self, max_length, min_length, type=False, do_sample=False):\n",
    "        \"\"\" Summarises text inputs \"\"\"\n",
    "\n",
    "        if type:\n",
    "            trained = type\n",
    "        else:\n",
    "            trained = 'xsum'\n",
    "        self.summarizer = pipeline('summarization', model=f'facebook/bart-large-{trained}')\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.do_sample = do_sample\n",
    "        self.summarised_docs = {}\n",
    "\n",
    "    def summarise_docs(self, docs):\n",
    "        \"\"\" \n",
    "        max_length (int): Maximum length of the generated summary.\n",
    "        min_length (int): Minimum length of the generated summary.\n",
    "        do_sample (bool): Whether to use greedy sampling when generating summaries.\n",
    "        \n",
    "        Returns: dict of summaries per ref\n",
    "        \"\"\"\n",
    "        \n",
    "        for ref, keys in docs.items():\n",
    "            \n",
    "            # add ref to dict\n",
    "            if ref not in self.summarised_docs:\n",
    "                    self.summarised_docs[ref] = {}\n",
    "            \n",
    "            # combine text\n",
    "            text = self.combine(keys)\n",
    "            \n",
    "            # if extra information exists\n",
    "            if len(keys) > 2:\n",
    "                \n",
    "                # TF-IDF and cosine similarity to find relevant sentences\n",
    "                short_desc = keys[\"DESCRIPTION\"]\n",
    "                relevant_sentences = self.extract_relevant_sentences(short_desc, sent_tokenize(text))\n",
    "\n",
    "                # combine relevant sentences\n",
    "                relevant_text = ' '.join(relevant_sentences)\n",
    "\n",
    "                # If extra information exists\n",
    "                if relevant_text:\n",
    "                    try:\n",
    "                        summary = self.summarizer(relevant_text, max_length=self.max_length, min_length=self.min_length, do_sample=self.do_sample)\n",
    "                        self.summarised_docs[ref][\"summary\"] = summary[0]['summary_text']\n",
    "                        self.summarised_docs[ref][\"method\"] = \"summarised\"\n",
    "                        print(f\"{ref} summarised\")\n",
    "                    except:\n",
    "                        print(f\"Error {ref}\")\n",
    "                        continue\n",
    "\n",
    "            # don't summarise if no specification documents\n",
    "            else:\n",
    "                self.summarised_docs[ref][\"summary\"] = text\n",
    "                self.summarised_docs[ref][\"method\"] = \"not summarised\"\n",
    "                print(f\"{ref} not summarised\")\n",
    "        \n",
    "        return self.summarised_docs\n",
    "        \n",
    "    def summary(self, ref):\n",
    "        \"\"\" Return summaries from input ref \"\"\"\n",
    "        return print(self.summarised_docs[ref])\n",
    "    \n",
    "    def extract_relevant_sentences(self, short_desc, text_list):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        short_desc_vectors = vectorizer.fit_transform([short_desc])\n",
    "        full_text_vectors = vectorizer.transform(text_list)\n",
    "\n",
    "        cosine_similarities = cosine_similarity(short_desc_vectors, full_text_vectors)\n",
    "\n",
    "        # sort sentences by cosine similarity and select top n\n",
    "        num_sentences_to_select = min(10, len(text_list))  # selecting top 10 to be summarised\n",
    "        selected_indices = cosine_similarities.argsort()[0][-num_sentences_to_select:]\n",
    "\n",
    "        relevant_sentences = [text_list[i] for i in selected_indices]\n",
    "\n",
    "        return relevant_sentences\n",
    "    \n",
    "    def combine(self, keys):\n",
    "        text_combined = \"\" \n",
    "        for key, text in keys.items():\n",
    "            if text is not None:\n",
    "                text_combined += text\n",
    "        return text_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82c73c-d1e6-4cc0-b995-f547b4af22a4",
   "metadata": {},
   "source": [
    "# Read Pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8923f65-4b19-402a-988a-b2ee81741f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def pickler(path):\n",
    "    pickles_read = {}\n",
    "    pickles_unread = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.pickle'):\n",
    "            file_path = os.path.join(path, file)\n",
    "            ref = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            try:\n",
    "                with open(file_path, \"rb\") as data:\n",
    "                    pickles_read[ref] = pickle.load(data)\n",
    "            except:\n",
    "                pickles_unread.append(file)\n",
    "    \n",
    "    return pickles_read, pickles_unread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21588d-1a1c-4e25-904a-ae068af79dbd",
   "metadata": {},
   "source": [
    "# Summarise Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e27dc-0aae-40f8-b2c2-086bb2057af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "pickle_path = \"C:/Users/Mitch/pickles/\"\n",
    "pickles, empty_pickles = pickler(pickle_path)\n",
    "\n",
    "random_pickle_keys = random.sample(list(pickles.keys()), 100)\n",
    "random_pickles = {key: pickles[key] for key in random_pickle_keys}\n",
    "\n",
    "#summariser = Summariser(max_length=100, min_length=50, do_sample=False) # can add type='cnn' to change what model trained on\n",
    "#summarised_docs = summariser.summarise_docs(random_pickles)\n",
    "\n",
    "#for ref, summary in summarised_docs.items():\n",
    "#    print(f\"Summary for {ref}:\\n{summary}\")\n",
    "#    print(\"<============================>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644a2a9-e95a-48b0-8c49-917613597cec",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c8809-9de1-4651-bf58-c18ad232a7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score\n",
    "\n",
    "class SummariserEvaluator:\n",
    "\n",
    "    def __init__(self, summariser, reference_summaries):\n",
    "        self.summariser = summariser\n",
    "        self.reference_summaries = reference_summaries\n",
    "        self.generated_summaries = {}\n",
    "\n",
    "    def generate_summaries(self):\n",
    "        all_summaries = self.summariser.summarise_docs(self.reference_summaries)\n",
    "    \n",
    "        # get those that were actually summarized\n",
    "        self.generated_summaries = {ref: keys[\"summary\"] for ref, keys in all_summaries.items() if keys.get(\"method\") == 'summarised'}\n",
    "        self.reference_summaries = {ref: keys[\"DESCRIPTION\"] for ref, keys in self.reference_summaries.items() if ref in self.generated_summaries}\n",
    "        \n",
    "    def evaluate_summaries(self):\n",
    "        if not self.generated_summaries:\n",
    "            return None\n",
    "\n",
    "        reference_list = list(self.reference_summaries.values())\n",
    "        generated_list = list(self.generated_summaries.values())\n",
    "        \n",
    "        # ROUGE scores\n",
    "        rouge = Rouge()\n",
    "        rouge_scores = rouge.get_scores(reference_list, generated_list, avg=True)\n",
    "\n",
    "        # BLEU score\n",
    "        bleu_scores = []\n",
    "        for ref, gen in zip(reference_list, generated_list):\n",
    "            bleu = sentence_bleu([ref], gen)\n",
    "            bleu_scores.append(bleu)\n",
    "\n",
    "        # BERT score\n",
    "        P, R, F1 = score(reference_list, generated_list, lang='en')\n",
    "\n",
    "        return {\n",
    "            'ROUGE': rouge_scores,\n",
    "            'BLEU': sum(bleu_scores) / len(bleu_scores),\n",
    "            'BERT_SCORE_P': P.mean().item(),\n",
    "            'BERT_SCORE_R': R.mean().item(),\n",
    "            'BERT_SCORE_F1': F1.mean().item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2c33b-9e93-4d21-8fc5-13dc462ed443",
   "metadata": {},
   "source": [
    "We may not want very high scores given that our task is not to replicate the short description, but improve upon it. As such, higher bert scores are desirable given that they check semantic relationships between the summaries. Medium rouge and bleu scores are probably expected given that generated summaries should be different and have more context. This is very particular to our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e0e23-215a-467b-9148-bdeff7388e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summariser = Summariser(max_length=100, min_length=50, do_sample=False) \n",
    "evaluator = SummariserEvaluator(summariser, random_pickles)\n",
    "evaluator.generate_summaries()\n",
    "\n",
    "scores = evaluator.evaluate_summaries()\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
