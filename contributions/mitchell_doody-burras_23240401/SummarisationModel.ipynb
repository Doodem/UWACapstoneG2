{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5616c483-731c-4ad8-a50e-3b0421d16b05",
   "metadata": {},
   "source": [
    "# Load Summarisation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa73c6f-9895-4451-a685-5ad602a1aa3b",
   "metadata": {},
   "source": [
    "# Method 1: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5adc3f7-3972-43a5-83f1-ac5e0a25b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class Summariser:\n",
    "\n",
    "    def __init__(self, max_length, min_length, chunk_size, type=False, do_sample=False):\n",
    "        \"\"\" Summarises text inputs \"\"\"\n",
    "\n",
    "        if type:\n",
    "            trained = type\n",
    "        else:\n",
    "            trained = 'xsum'\n",
    "        self.summarizer = pipeline('summarization', model=f'facebook/bart-large-{trained}')\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.do_sample = do_sample\n",
    "        self.chunk_size = chunk_size\n",
    "        self.summarised_docs = {}\n",
    "\n",
    "    def summarise_docs(self, docs):\n",
    "        \"\"\" \n",
    "        max_length (int): Maximum length of the generated summary.\n",
    "        min_length (int): Minimum length of the generated summary.\n",
    "        do_sample (bool): Whether to use greedy sampling when generating summaries.\n",
    "        chunk_size (int): Max size of chunk if need to summarise chunks seperately becuase input too large.\n",
    "        \n",
    "        Returns: dict of summaries per ref\n",
    "        \"\"\"\n",
    "        \n",
    "        for ref, keys in docs.items():\n",
    "            # combine text\n",
    "            text = self.combine(keys)\n",
    "            \n",
    "            # if extra information exists\n",
    "            if len(keys) > 2:\n",
    "            \n",
    "                # If chunking is enabled and text is too long\n",
    "                if self.chunk_size and len(text) > self.chunk_size:\n",
    "                    chunks = self.chunk_text(text, self.chunk_size)\n",
    "                    summary_text = \"\"\n",
    "                    for chunk in chunks:\n",
    "                        try:\n",
    "                            summary = self.summarizer(chunk, max_length=self.max_length, min_length=self.min_length, do_sample=self.do_sample)\n",
    "                            summary_text += summary[0]['summary_text'] + \"\\n\"\n",
    "                        except:\n",
    "                            print(f\"Chunked document for '{ref}' is too long\")\n",
    "                            continue\n",
    "                    self.summarised_docs[ref] = summary_text.strip()\n",
    "                    print(f\"{ref} summarised in chunks\")\n",
    "                    \n",
    "                # summarise as normal if extra information not long\n",
    "                else:\n",
    "                    try:\n",
    "                        summary = self.summarizer(text, max_length=self.max_length, min_length=self.min_length, do_sample=self.do_sample)\n",
    "                        self.summarised_docs[ref] = summary[0]['summary_text']\n",
    "                        print(f\"{ref} summarised\")\n",
    "                    except:\n",
    "                        print(f\"Error occured while processing {ref}!!!\")\n",
    "                        continue\n",
    "            # don't summarise title and short desc\n",
    "            else:\n",
    "                self.summarised_docs[ref] = text\n",
    "                print(f\"{ref} not summarised\")\n",
    "        \n",
    "        return self.summarised_docs\n",
    "        \n",
    "    def summary(self, ref):\n",
    "        \"\"\" Return summaries from input ref \"\"\"\n",
    "        return print(self.summarised_docs[ref])\n",
    "    \n",
    "    def combine(self, keys):\n",
    "        text = \"\"\n",
    "        for key, text in keys.items():\n",
    "            if text is not None:\n",
    "                text += text\n",
    "        return text\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size):\n",
    "        \"\"\" Split long text into chunks of specified size to summarise seperately \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            start = end\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe46053-7b9a-4e76-b777-7f743aba4823",
   "metadata": {},
   "source": [
    "# Method 2: Extract relevant text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6be3c-f1dd-41a6-8d70-a36f6a714cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "class Summariser:\n",
    "\n",
    "    def __init__(self, max_length, min_length, type=False, do_sample=False):\n",
    "        \"\"\" Summarises text inputs \"\"\"\n",
    "\n",
    "        if type:\n",
    "            trained = type\n",
    "        else:\n",
    "            trained = 'xsum'\n",
    "        self.summarizer = pipeline('summarization', model=f'facebook/bart-large-{trained}')\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.do_sample = do_sample\n",
    "        self.summarised_docs = {}\n",
    "\n",
    "    def summarise_docs(self, docs):\n",
    "        \"\"\" \n",
    "        max_length (int): Maximum length of the generated summary.\n",
    "        min_length (int): Minimum length of the generated summary.\n",
    "        do_sample (bool): Whether to use greedy sampling when generating summaries.\n",
    "        \n",
    "        Returns: dict of summaries per ref\n",
    "        \"\"\"\n",
    "        \n",
    "        for ref, keys in docs.items():\n",
    "            # combine text\n",
    "            text = self.combine(keys)\n",
    "            \n",
    "            # if extra information exists\n",
    "            if len(keys) > 2:\n",
    "                \n",
    "                # TF-IDF and cosine similarity to find relevant sentences\n",
    "                short_desc = keys[\"DESCRIPTION\"]\n",
    "                relevant_sentences = self.extract_relevant_sentences(short_desc, sent_tokenize(text))\n",
    "\n",
    "                # combine relevant sentences\n",
    "                relevant_text = ' '.join(relevant_sentences)\n",
    "                \n",
    "                print(f\"SHORT DESCRIPTION = {short_desc}\")\n",
    "                print(f\"RELEVANT EXTRACT = {relevant_text}\")\n",
    "\n",
    "                # If extra information exists\n",
    "                if relevant_text:\n",
    "                    try:\n",
    "                        summary = self.summarizer(relevant_text, max_length=self.max_length, min_length=self.min_length, do_sample=self.do_sample)\n",
    "                        self.summarised_docs[ref] = summary[0]['summary_text']\n",
    "                        print(f\"{ref} summarised\")\n",
    "                    except:\n",
    "                        print(f\"Document for '{ref}' is most likely too long\")\n",
    "                        continue\n",
    "\n",
    "            # don't summarise if no relevant text is found\n",
    "            else:\n",
    "                self.summarised_docs[ref] = text\n",
    "                print(f\"{ref} not summarised\")\n",
    "        \n",
    "        return self.summarised_docs\n",
    "        \n",
    "    def summary(self, ref):\n",
    "        \"\"\" Return summaries from input ref \"\"\"\n",
    "        return print(self.summarised_docs[ref])\n",
    "    \n",
    "    def extract_relevant_sentences(self, short_desc, text_list):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        short_desc_vectors = vectorizer.fit_transform([short_desc])\n",
    "        full_text_vectors = vectorizer.transform(text_list)\n",
    "\n",
    "        cosine_similarities = cosine_similarity(short_desc_vectors, full_text_vectors)\n",
    "\n",
    "        # sort sentences by cosine similarity and select top n\n",
    "        num_sentences_to_select = min(10, len(text_list))  # selecting top 10 to be summarised\n",
    "        selected_indices = cosine_similarities.argsort()[0][-num_sentences_to_select:]\n",
    "\n",
    "        relevant_sentences = [text_list[i] for i in selected_indices]\n",
    "\n",
    "        return relevant_sentences\n",
    "    \n",
    "    def combine(self, keys):\n",
    "        text_combined = \"\" \n",
    "        for key, text in keys.items():\n",
    "            if text is not None:\n",
    "                text_combined += text\n",
    "        return text_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82c73c-d1e6-4cc0-b995-f547b4af22a4",
   "metadata": {},
   "source": [
    "# Read Pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8923f65-4b19-402a-988a-b2ee81741f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def pickler(path):\n",
    "    pickles_read = {}\n",
    "    pickles_unread = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.pickle'):\n",
    "            file_path = os.path.join(path, file)\n",
    "            ref = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            try:\n",
    "                with open(file_path, \"rb\") as data:\n",
    "                    pickles_read[ref] = pickle.load(data)\n",
    "            except:\n",
    "                pickles_unread.append(file)\n",
    "    \n",
    "    return pickles_read, pickles_unread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21588d-1a1c-4e25-904a-ae068af79dbd",
   "metadata": {},
   "source": [
    "# Summarise Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e27dc-0aae-40f8-b2c2-086bb2057af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "pickle_path = \"C:/Users/Mitch/pickles/\"\n",
    "pickles, empty_pickles = pickler(pickle_path)\n",
    "\n",
    "random_pickle_keys = random.sample(list(pickles.keys()), 10)\n",
    "random_pickles = {key: pickles[key] for key in random_pickle_keys}\n",
    "\n",
    "summariser = Summariser(max_length=100, min_length=50, do_sample=False) # can add type='cnn' to change what model trained on\n",
    "summarised_docs = summariser.summarise_docs(random_pickles)\n",
    "\n",
    "for ref, summary in summarised_docs.items():\n",
    "    print(f\"Summary for {ref}:\\n{summary}\")\n",
    "    print(\"<============================>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f4289-ae3f-46e4-b5b8-ecdca57703bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
