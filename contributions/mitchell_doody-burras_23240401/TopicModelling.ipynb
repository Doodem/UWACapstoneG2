{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e09a77-981e-4a11-aa08-735124a319c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "def pickler(path):\n",
    "    pickles_read = {}\n",
    "    pickles_unread = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.pickle'):\n",
    "            file_path = os.path.join(path, file)\n",
    "            ref = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            try:\n",
    "                with open(file_path, \"rb\") as data:\n",
    "                    pickles_read[ref] = pickle.load(data)\n",
    "            except:\n",
    "                pickles_unread.append(file)\n",
    "    \n",
    "    return pickles_read, pickles_unread\n",
    "\n",
    "pickle_path = \"C:/Users/Mitch/pickles/\"\n",
    "pickles, empty_pickles = pickler(pickle_path)\n",
    "\n",
    "random_pickle_keys = random.sample(list(pickles.keys()), 100)\n",
    "random_pickles = {key: pickles[key] for key in random_pickle_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c40f770-1af9-4298-ad45-517623b59d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine(dictionary):\n",
    "    combined_text = []\n",
    "    for key in dictionary:\n",
    "        inner_dict = dictionary[key]\n",
    "        document_text = \"\"\n",
    "        for inner_key in inner_dict:\n",
    "            inner_text = inner_dict[inner_key]\n",
    "            if inner_text is not None:\n",
    "                document_text += inner_text\n",
    "        combined_text.append(document_text)\n",
    "    return combined_text\n",
    "\n",
    "corpus = combine(random_pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0030b081-e9d1-4e30-afb3-5b92f3ef96d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mitch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9bddd3d-3fbe-4e5f-bc0a-0645804c758f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [3, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only include those words that appear in less than 80% of the document and appear in at least 2 documents. \n",
    "cv = CountVectorizer(max_df=0.8, min_df=2, stop_words=stopwords.words('english'), lowercase=True)\n",
    "x = cv.fit_transform(corpus)\n",
    "x.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a33e16-cc7a-4ac8-a0b4-7da0362aa26e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x16347 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 91647 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each of the 10 documents is represented as a 1908 dimensional vector, which means that our vocabularly has 1908 words\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a05921-beee-4ce4-924a-bf2ed1fbf9fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0003', ..., 'zoning', 'zoological', 'zs'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8153f562-18e4-4016-bd47-9dea71767b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we pick 5 topics that we want out text to be divided into\n",
    "LDA = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "LDA.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0ba3c8-d55e-43a4-aed6-57790f764f71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echnical\n",
      "ependent\n",
      "pigmented\n",
      "attributed\n",
      "design\n",
      "236\n",
      "ichever\n",
      "2012\n",
      "carbonate\n",
      "plan1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# fetch 10 random words from our vocabularly\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(cv.get_feature_names_out()))\n",
    "    print(cv.get_feature_names_out()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca96471-4cfa-4517-8132-87f8bc6aad86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provide\n",
      "services\n",
      "new\n",
      "works\n",
      "contractor\n",
      "tender\n",
      "01\n",
      "block\n",
      "shall\n",
      "existing\n"
     ]
    }
   ],
   "source": [
    "# get 10 words with highest probability for the first topic\n",
    "first_topic = LDA.components_[0]\n",
    "top_topic_words = first_topic.argsort()[-10:]\n",
    "for i in top_topic_words:\n",
    "    print(cv.get_feature_names_out()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73559d73-e564-4c15-9e0c-a71828f113cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['provide', 'services', 'new', 'works', 'contractor', 'tender', '01', 'block', 'shall', 'existing']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['requirements', 'design', 'part', 'services', 'respondent', 'general', 'contract', 'fire', 'shall', 'customer']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['shall', 'must', 'site', 'general', 'works', 'work', 'principal', 'tender', 'contract', 'contractor']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['department', 'electrical', 'cable', 'services', 'works', 'general', 'system', 'equipment', 'provide', 'shall']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['technical', 'door', 'services', 'maintenance', 'mm', 'principal', 'required', 'works', 'contractor', 'must']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['mid', 'tier', 'laser', 'cockburn', 'fleet', 'facilitation', 'denny', 'assist', 'avenue', 'communities']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['work', 'must', 'tenderer', 'clause', 'works', 'principal', 'tender', 'contract', 'shall', 'contractor']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['aug', '50', '10', '2017', '00', 'shall', '05', 'mg', 'kg', 'ne']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['2019', 'department', 'australia', 'lift', 'wge', '25government', 'management', 'workswestern', 'midland', '__________']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['may', 'works', 'tenderer', '10', 'water', 'superintendent', 'contract', 'principal', 'contractor', 'shall']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we print out the 10 words with the highest probabilities for all the five topics we choose\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([cv.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f69bba-3355-49ef-9424-a2e53c17e850",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 is assigned to topic 2\n",
      "Document 1 is assigned to topic 2\n",
      "Document 2 is assigned to topic 9\n",
      "Document 3 is assigned to topic 1\n",
      "Document 4 is assigned to topic 1\n",
      "Document 5 is assigned to topic 3\n",
      "Document 6 is assigned to topic 4\n",
      "Document 7 is assigned to topic 9\n",
      "Document 8 is assigned to topic 4\n",
      "Document 9 is assigned to topic 4\n",
      "Document 10 is assigned to topic 6\n",
      "Document 11 is assigned to topic 6\n",
      "Document 12 is assigned to topic 5\n",
      "Document 13 is assigned to topic 6\n",
      "Document 14 is assigned to topic 9\n",
      "Document 15 is assigned to topic 6\n",
      "Document 16 is assigned to topic 1\n",
      "Document 17 is assigned to topic 8\n",
      "Document 18 is assigned to topic 6\n",
      "Document 19 is assigned to topic 2\n",
      "Document 20 is assigned to topic 6\n",
      "Document 21 is assigned to topic 6\n",
      "Document 22 is assigned to topic 1\n",
      "Document 23 is assigned to topic 2\n",
      "Document 24 is assigned to topic 4\n",
      "Document 25 is assigned to topic 3\n",
      "Document 26 is assigned to topic 2\n",
      "Document 27 is assigned to topic 2\n",
      "Document 28 is assigned to topic 6\n",
      "Document 29 is assigned to topic 2\n",
      "Document 30 is assigned to topic 9\n",
      "Document 31 is assigned to topic 1\n",
      "Document 32 is assigned to topic 6\n",
      "Document 33 is assigned to topic 2\n",
      "Document 34 is assigned to topic 1\n",
      "Document 35 is assigned to topic 6\n",
      "Document 36 is assigned to topic 1\n",
      "Document 37 is assigned to topic 1\n",
      "Document 38 is assigned to topic 2\n",
      "Document 39 is assigned to topic 1\n",
      "Document 40 is assigned to topic 6\n",
      "Document 41 is assigned to topic 5\n",
      "Document 42 is assigned to topic 2\n",
      "Document 43 is assigned to topic 2\n",
      "Document 44 is assigned to topic 7\n",
      "Document 45 is assigned to topic 6\n",
      "Document 46 is assigned to topic 6\n",
      "Document 47 is assigned to topic 9\n",
      "Document 48 is assigned to topic 9\n",
      "Document 49 is assigned to topic 6\n",
      "Document 50 is assigned to topic 0\n",
      "Document 51 is assigned to topic 2\n",
      "Document 52 is assigned to topic 0\n",
      "Document 53 is assigned to topic 1\n",
      "Document 54 is assigned to topic 2\n",
      "Document 55 is assigned to topic 1\n",
      "Document 56 is assigned to topic 6\n",
      "Document 57 is assigned to topic 9\n",
      "Document 58 is assigned to topic 1\n",
      "Document 59 is assigned to topic 9\n",
      "Document 60 is assigned to topic 2\n",
      "Document 61 is assigned to topic 4\n",
      "Document 62 is assigned to topic 8\n",
      "Document 63 is assigned to topic 6\n",
      "Document 64 is assigned to topic 2\n",
      "Document 65 is assigned to topic 9\n",
      "Document 66 is assigned to topic 1\n",
      "Document 67 is assigned to topic 5\n",
      "Document 68 is assigned to topic 6\n",
      "Document 69 is assigned to topic 9\n",
      "Document 70 is assigned to topic 1\n",
      "Document 71 is assigned to topic 1\n",
      "Document 72 is assigned to topic 1\n",
      "Document 73 is assigned to topic 9\n",
      "Document 74 is assigned to topic 5\n",
      "Document 75 is assigned to topic 3\n",
      "Document 76 is assigned to topic 2\n",
      "Document 77 is assigned to topic 6\n",
      "Document 78 is assigned to topic 9\n",
      "Document 79 is assigned to topic 6\n",
      "Document 80 is assigned to topic 9\n",
      "Document 81 is assigned to topic 2\n",
      "Document 82 is assigned to topic 3\n",
      "Document 83 is assigned to topic 7\n",
      "Document 84 is assigned to topic 6\n",
      "Document 85 is assigned to topic 9\n",
      "Document 86 is assigned to topic 7\n",
      "Document 87 is assigned to topic 9\n",
      "Document 88 is assigned to topic 7\n",
      "Document 89 is assigned to topic 6\n",
      "Document 90 is assigned to topic 5\n",
      "Document 91 is assigned to topic 1\n",
      "Document 92 is assigned to topic 2\n",
      "Document 93 is assigned to topic 9\n",
      "Document 94 is assigned to topic 2\n",
      "Document 95 is assigned to topic 9\n",
      "Document 96 is assigned to topic 3\n",
      "Document 97 is assigned to topic 6\n",
      "Document 98 is assigned to topic 5\n",
      "Document 99 is assigned to topic 1\n"
     ]
    }
   ],
   "source": [
    "document_topic_distributions = LDA.transform(x)\n",
    "\n",
    "# assign tenders back to the most likely topic\n",
    "assigned_topics = [np.argmax(doc_topic_dist) for doc_topic_dist in document_topic_distributions]\n",
    "for i, assigned_topic in enumerate(assigned_topics):\n",
    "    print(f\"Document {i} is assigned to topic {assigned_topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d722b-18b9-43c0-a65a-d235d8bbc345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
